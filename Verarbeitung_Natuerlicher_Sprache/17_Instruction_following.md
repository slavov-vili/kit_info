# Evaluating LLMs
1. Task-dependent evaluation
1. Challenges of evaluating large language model
    - Contamination = model might have already seen the test set
1. General Language Understanding Evaluation (GLUE) benchmark
    - TODO: tasks
1. Conversational benchmarks
    - TODO: tasks
    - multi-turn = same initial prompt, but ask more about it



# Instruction Training
1. Motivation
1. Challenges
    - Model not trained to help, but to predict
1. Instruction Finetuning

